[meta
  title:"The Beginner's Guide to Dimensionality Reduction"
  description:"Welcome to our interactive tutorial." /]


[data name:"images" source:"met-with-coordinates.csv"  /]
[var name:"scrollState" value:"loading" /]

[var name:"widthWeight" value:0 /]
[var name:"heightWeight" value:0 /]
[var name:"weightWeight" value:0 /]
[var name:"endDateWeight" value:0 /]
[var name:"brightnessWeight" value:1 /]
[var name:"showHilbert" value:false /]
[var name:"algorithm" value:"" /]
[var name:"selectedArtwork" value:`null ` /]

[Fixed]
  [DRComponent
    images:images
    state:scrollState
    showHilbert:showHilbert
    algorithm:algorithm
    selectedArtwork:selectedArtwork
    weights:`{
      // 'Width (cm)': widthWeight,
      // 'Height (cm)': heightWeight,
      'Object End Date': 1 - brightnessWeight,
      'brightness_avg_perceived': brightnessWeight
    }`
      /]

[/Fixed]

[Scroller currentState:scrollState]


  [Step state:"loading"]


    [Header
      title:"The Beginner's Guide to Dimensionality Reduction"
      subtitle:"Welcome to our interactive tutorial."
      date:"July 12, 2018"
      authors:`[
        { name: "Matthew Conlen", link: "https://twitter.com/mathisonian" },
        { name: "Fred Hohman", link: "https://twitter.com/fredhohman" }
      ]` /]

    [var name:"loadStatus" value:"initial" /]
    [StartButton state:scrollState status:loadStatus /]

    [Conditional if:`loadStatus === 'loaded'`]
      Dimensionality reduction is a powerful
      technique used by data scientists to look for hidden structure in data.  The method is used by a variety of research and applications to visualize datasets to identify patterns and clusters of similar or dissimlar data.

      For example, Google's [Embedding Projector](https://projector.tensorflow.org/) let's you view a
      number of high-dimensional datasets projected onto two or three dimensions. The idea is that
      even though data is displayed in a lower dimension, structures present in higher dimensions are maintained, at least roughly.

      Dimensionality reduction algorithms work by inferring a function to describe the structure of "unlabeled" data, i.e. data that has no preexisting categorization. However, many of the commonly used algorithms produce complex *embeddings* and, since all considered data are unlabelled, it is difficult to evaluate the results of the structures produced by such algorithms.

      Read on to learn what exactly these embeddings are, take a look at some simple algorithms, and
      view a comparison of three of the most popular dimensionality reduction algorithms used today.

    [/Conditional]

  [/Step]


  [Step state:"reveal"]

    # First, some art

    Your browser has just loaded information about
    1,517 artworks from the collection at the [Metropolitan Museum of Art](https://www.metmuseum.org/). The museum has publicly released a large dataset about their collection [Cite reference:"met"/].

    *Hover over an artwork to see its details.*

    [br/]

    Each artwork includes basic metadata, such as its title, artist, date made, medium, and dimensions.
    Below are 10 random artworks from the dataset.

    [br/]

    [Table
      data:`images.slice(0, 10)`
      columns:`[
        { Header: 'Year', accessor: 'Object End Date' },
        { Header: 'Title',  accessor: 'Title'},
        { Header: 'Artist', accessor: 'Artist Display Name'}]` /]

    [br/]

    Computer scientists like to call metadata for each data point (artwork) *features*.

  [/Step]


  [Step state:"1d"]

    # Projecting onto a line

    If you store the features in a *vector*, you can think of the data as existing in
    a high-dimensional space.

    We often want to visualize the data, however we cannot show all the dimensions at once.
    Instead we can choose to project the data into a lower dimension (oftentimes 1D, 2D, or 3D) that can be visualized.
    This projection sometimes is also called an embedding.
    // of the data into a lower dimensional space.

    *Remember points are 0D, lines are 1D, planes are 2D, etc.*

    [br/]

    Let's compute a 1 dimensional embedding.
    This means we will take each artwork and compute one number to describe it
    instead of all of the features we showed earlier in the table.
    A benefit of reducing to 1D is that we can sort the numbers, and thus the artworks, on a line.


    [Projection images:images src:"./static/images/projection.svg" /]

    Let's reduce using *average pixel brightness* of each artwork, and then sort these values.

    Notice that the darkest images appear at the top and the brightest images on the bottom!

  [/Step]


  [Step state:"reset"]

    # For the mathematically inclined

    We can formulate dimensionality mathematically in the context of a given dataset.

    Consider a dataset typically represented as a matrix
    [Equation display:false]X[/Equation], where
    [Equation display:false]X[/Equation]
    is of size
    [Equation display:false]m \times n[/Equation], where
    [Equation display:false]m[/Equation] is the number of rows of
    [Equation display:false]X[/Equation], and
    [Equation display:false]n[/Equation] is the number of columns.
    Typically, the rows are *data points* and the columns are *features*.
    Dimensionality reduction will reduce the number of features of each data point, turning
    [Equation display:false]X[/Equation] into a new matrix,
    [Equation display:false]X'[/Equation], of size
    [Equation display:false]m \times d[/Equation], where
    [Equation display:false]d < n[/Equation].
    As already stated, for visualization, we typically set
    [Equation display:false]d[/Equation] to be 1, 2 or 3.

    For example, say [Equation display:false]m=n[/Equation], that is
    [Equation display:false]X[/Equation] is a square matrix.
    Performing dimensionality reduction on
    [Equation display:false]X[/Equation] and setting
    [Equation display:false]d=2[/Equation] will change it from a square matrix to a tall, rectangular matrix.

    [Equation display:true]
    X =
    \begin{bmatrix}
    x & x & x \\
    x & x & x \\
    x & x & x
    \end{bmatrix}
    \implies
    \begin{bmatrix}
    x' & x' \\
    x' & x' \\
    x' & x'
    \end{bmatrix}
    = X'
    [/Equation]

    Here,
    [Equation display:false]x[/Equation] and
    [Equation display:false]x'[/Equation] just represent nonzero values of a matrix.

    Each data point only has two features now, i.e., each data point has been reduced from a 3 dimensional vector to a 2 dimensional vector.

  [/Step]


  [Step state:"hilbert-brightness"]

    # Embedding data in two dimensions

    Instead of reducing to 1D, let's add another dimension and embed the artworks in 2D.

    Here we take the same brightness feature and use it to position the artworks in 2D space instead of 1D.
    The pieces have more room to spread out.

    But this is not the only way to embed the artworks in 2D space.
    In fact, there are many, and some projections are more useful than others.

    Use these sliders to toggle the influence that the brightness and artwork age
    features have in determining the artworks' embedding positions.

    [div style:`{display: 'flex', flexAlign: 'row', textAlign: 'center'}`]

      [div style:`{margin: '0 auto'}`]
      *Artwork Age*
      [Range value:brightnessWeight min:0 max:1 step:0.005 /]
      *Brightness*
      [/div]

    [/div]

    [var name:"showHilbertDetails" value:false /]

    [conditional if:`!showHilbertDetails ` ]
      [button onClick:` showHilbertDetails = true ` ]
        Show technical details
      [/button]
    [/conditional]

    [conditional if:showHilbertDetails ]

    The embedding you see here is actually a linear 1D embedding, whose resulting scalar is then
    mapped on a space-filling Hilbert curve to give the illusion of a 2D embedding.

    Each artwork's 1D reduced projection is computed by a linear combination of the three features above.

    Let [Equation display:false]a[/Equation] be a given artwork, and let each slider's value be a weight
    [Equation display:false]w_{i}[/Equation].
    We will compute
    [Equation display:false]a'[/Equation], the scalar projection of
    [Equation display:false]a[/Equation] into
    [Equation display:false]\mathbb{R}[/Equation].

    [Equation display:true]
    a' = (a_{\text{brightness}} \times w_{\text{brightness}}) + (a_{\text{age}} \times w_{\text{age}})
    [/Equation]

    We then jitter the artworks to prevent excessive overlap.

    [button onClick:`showHilbert = !showHilbert `]Toggle hilbert curve[/button]

    [/conditional]
  [/Step]

  [Step state:"algorithms"]

    # Real-world algorithms

    We just showed an example of a user-driven embedding, where the exact influence of each feature is known in the embedding.
    However, you may have noticed that its hard to find meaningful combinations of feature weights.
    Luckily this is something that computers are very good at: searching for some optimal combination of numbers.


    Below are the three discussed algorithms run on the artworks.
    Click each button to toggle between the resulting embeddings.

    [div className:"panel" ]
    [div style:`{display: 'flex', flexAlign: 'row', textAlign: 'center'}`]

      [button className:`algorithm === 'pca' ? 'selected' : ''` onClick:`algorithm = "pca" `]
        PCA
      [/button]

      [button className:`algorithm === 'mctsne' ? 'selected' : ''` onClick:`algorithm = "mctsne" `]
        t-SNE
      [/button]

      [button className:`algorithm === 'umap' ? 'selected' : ''` onClick:`algorithm = "umap" `]
        UMAP
      [/button]
    [/div]

    [conditional if:`algorithm === 'pca' ` ]
      ## Principal component analysis

      Pros:
* Computationally cheap.
* Can save embedding model to then project new data points into the reduced space.

      Cons:
* Linear reduction, resulting embedding is not as clustered as other algorithm.

    [/conditional]
    [conditional if:`algorithm === 'mctsne' ` ]
      ## t-Distributed stochastic neighbor embedding

      Pros:
* Produces highly clustered, visually striking embeddings.
* Non-linear reduction, captures more structure in data.

      Cons:
* More computationally expensive.
* Requires setting hyperparameters that influence quality of the embedding.
* Non-deterministic algorithm.

    [/conditional]
    [conditional if:`algorithm === 'umap' ` ]
      ## Uniform manifold approximation and projection

      Pros:
* Non-linear reduction that is computationally faster than t-SNE.
* Often performs better at finding global structure of the data as well as preserving local neighbor relations.
* Solid theoretical foundations in manifold learning.

      Cons:
* New, less prevalent algorithm.
* Requires setting hyperparameters that influence quality of the embedding.

    [/conditional]
[/div]


    There are many algorithms that compute a dimensionality reduction of a dataset.
    Simpler algorithms such as principal component analysis (PCA) maximize the variance in the data to produce the best possible embedding.
    More complicated algorithms, such as t-distributed stochastic neighbor embedding (t-SNE) [Cite reference:"maaten2008visualizing"/],
    iteratively produce highly clustered embeddings.
    Unfortunately, whereas before each feature's influence was explicitly known,
    one must relinquish control to a machine to determine the best embedding—
    that means that it is not clear what features of the data are used to compute the embedding.

    Dimensionality reduction, and more broadly the field of unsupervised learning, is an active area of research where researchers are
    developing new techniques to create better embeddings.
    A new technique, uniform manifold approximation and projection (UMAP) [Cite reference:"mcinnes2018umap"/],
    is a non-linear reduction that aims to create visually striking embeddings fast, scaling to larger datasets.


  [/Step]

  [Step]
    # Try it for yourself

    Dimensionality reduction is a powerful tool to better understand high-dimensional data.
    If you have your own dataset and wish to visualize it using dimensionality reduction, there are a number of different algorithms and
    implementations available.
    In Python, the scikit-learn package provides APIs for many
    [unsupervised dimensionality reduction](http://scikit-learn.org/stable/modules/unsupervised_reduction.html) algorithms,
    as well as [manifold learning](http://scikit-learn.org/stable/modules/manifold.html): an approach to non-linear dimensionality reduction.

    Regarding the three algorithms dicussed above, you can find Python implementations of the algorithms we used for the artworks here:
    [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html),
    [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html),
    and [UMAP](https://github.com/lmcinnes/umap).

    ### Acknowledgments
* This article was created using [Idyll](https://idyll-lang.org).
* The source code is available on [Github](https://github.com/mathisonian/dimensionality-reduction).

    [References /]

  [/Step]

[/Scroller]



[ArtworkDetails artwork:selectedArtwork /]
