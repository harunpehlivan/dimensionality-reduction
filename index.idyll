[meta
  title:"What is Dimensionality Reduction?"
  description:"An intuitive guide to the statistical technique." /]


[data name:"images" source:"met-with-coordinates.csv"  /]
[var name:"scrollState" value:"loading" /]

[var name:"widthWeight" value:0 /]
[var name:"heightWeight" value:0 /]
[var name:"weightWeight" value:0 /]
[var name:"endDateWeight" value:0 /]
[var name:"brightnessWeight" value:1 /]
[var name:"showHilbert" value:false /]
[var name:"algorithm" value:"" /]
[var name:"selectedArtwork" value:`null ` /]

[Fixed]
  [DRComponent
    images:images
    state:scrollState
    showHilbert:showHilbert
    algorithm:algorithm
    selectedArtwork:selectedArtwork
    weights:`{
      // 'Width (cm)': widthWeight,
      // 'Height (cm)': heightWeight,
      'Object End Date': endDateWeight,
      'brightness_avg_perceived': brightnessWeight
    }`
      /]

[/Fixed]

[Scroller currentState:scrollState]


  [Step state:"loading"]


    [Header
      title:"What is Dimensionality Reduction?"
      date:"July 12, 2018"
      authors:`[
        { name: "Matthew Conlen", link: "https://twitter.com/mathisonian" },
        { name: "Fred Hohman", link: "https://twitter.com/fredhohman" }
      ]` /]

    [StartButton state:scrollState /]
  [/Step]

  [Step state:"intro"]

  # The Intro

  [Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) is a common yet powerful
  technique used in data mining, machine learning, and now more broadly, AI research and applications
  that is used to visualize datasets to identify patterns and clusters of similar or dissimlar data.
  // cite some applications?

  More formally, the goal of dimsionality reduction, a subset of unsupervised learning,
  is to infer a function to describe the structure of "unlabeled" data, i.e. data that has no categorization.
  
  [br/]

  This presents a challenge for all dimsionality reduction techniques,
  since considered data is unlabelled, there is no straightforward way to evaluate the accuracy of the
  embedding or structure that is produced by the reduction algorithm.

  Furthermore, many of the commonly used algorithms produce puzzling *embeddings*: the output of dimensionality reduction.

  Here, we will show examples of what dimensionality produces on real data,
  present an interactive visualization of a simple reduction that allows a user to specify the rules of the embedding,
  and a comparison of three of the most popular dimensionality reduction algorithms used today.

  [/Step]

  [Step state:"reveal"]

    # The Art
    
    For our data, consider a collection of artworks from the [The Metropolitan Museum of Art](https://www.metmuseum.org/) [Cite reference:"met"/].
    
    [br/]

    We will use a subset of this data that contains 1,517 artworks.
    
    [br/]

    Hover over an artwork to see its details.

  [/Step]

  [Step state:"table"]

    # The Data

    Each artwork includes basic metadata, such as its title, artist, date made, medium, and dimensions.
    Below are 10 random artworks from the dataset.

    [br/]

    [Table
      data:`images.slice(0, 10)`
      columns:`['Object Name', 'Title', 'Artist Display Name']` /]

    [br/]

    Computer scientists like to call metadata for each data point (artwork) *features*.

  [/Step]


  [Step state:"1d"]

    # The Projection

    If you store the features in a *vector*, you can think of the data as existing in
    a high-dimensional space.

    We often want to visualize the data, however we cannot show all the dimensions at once.
    Instead we can choose to project the data into a lower dimension (oftentimes 1D, 2D, or 3D) that can be visualized.
    This projection sometimes is also called an embedding.
    // of the data into a lower dimensional space.

    // TODO - make this better
    [Svg src:"./static/images/projection.svg" /]

    Let's compute a 1 dimensional embedding.
    This means we will take each artwork and compute one number to describe it
    instead of all of the features we showed earlier in the table.
    A benefit of reducing to 1D is that we can sort the numbers, and thus the artworks, on a line.
    (Remember points are 0D, lines are 1D, planes are 2D, etc.)

    Let's reduce using *average pixel brightness* of each arwork, and then sort these values.
    
    Notice that the darkest images appear at the top and the brightest images on the bottom!

  [/Step]


  [Step state:"reset"]

    # The Math

    We can formulate dimensionality mathematically in the context of a given dataset.

    Consider a datseta typically represented as a matrix
    [Equation display:false]X[/Equation], where
    [Equation display:false]X[/Equation]
    is of size
    [Equation display:false]m \times n[/Equation], where
    [Equation display:false]m[/Equation] is the number of rows of
    [Equation display:false]X[/Equation], and
    [Equation display:false]n[/Equation] is the number of columns.
    Typically, the rows are *data points* and the columns are *features*.
    Dimesionality reduction will reduce the number of features of each data point, turning
    [Equation display:false]X[/Equation] into a new matrix,
    [Equation display:false]X'[/Equation], of size
    [Equation display:false]m \times d[/Equation], where
    [Equation display:false]d < n[/Equation].
    As already stated, for visualization, we typically set 
    [Equation display:false]d[/Equation] to be 1, 2 or 3.

    For example, say [Equation display:false]m=n[/Equation], that is
    [Equation display:false]X[/Equation] is a square matrix.
    Performing dimesionality reduction on
    [Equation display:false]X[/Equation] and setting 
    [Equation display:false]d=2[/Equation] will change it from a square matrix to a tall, rectangular matrix.

    [br /]
    [Equation display:true]
    X =
    \begin{bmatrix}
    x & x & x \\
    x & x & x \\
    x & x & x
    \end{bmatrix}
    \implies
    \begin{bmatrix}
    x' & x' \\
    x' & x' \\
    x' & x'
    \end{bmatrix}
    = X'
    [/Equation]

    Here,
    [Equation display:false]x[/Equation] and 
    [Equation display:false]x'[/Equation] just represent nonzero values of a matrix.

    Each data point only has two features now, i.e., each data point has been reduced from a 3 dimensional vector to a 2 dimensional vector.

  [/Step]


  [Step state:"hilbert-brightness"]
    # The Embedding

    Here we take the same brightness attribute and use it to position
    the artworks in 2D space instead of 1D.

    The pieces have more room to spread out.

    Some projections are more useful than others. Use these sliders
    to toggle the influence that variables have in determining
    the objects' position.

    [div style:`{display: 'flex', flexAlign: 'row', textAlign: 'center'}`]

    *Brightness*
    [Range value:brightnessWeight min:0 max:1 step:0.005 /]

    *Artwork Age*
    [Range value:endDateWeight min:0 max:1 step:0.005 /]

    *Height*
    [Range value:heightWeight min:0 max:1 step:0.005 /]

    [/div]

    [var name:"showHilbertDetails" value:false /]

    [conditional if:`!showHilbertDetails ` ]
      [button onClick:` showHilbertDetails = true ` ]
        Show technical details
      [/button]
    [/conditional]

    [conditional if:showHilbertDetails ]
      Say something about the hilbert curve.

      [button onClick:`showHilbert = !showHilbert `][Display value:`showHilbert ? 'Show' : 'Hide'` /]  hilbert[/button]

      !
    [/conditional]
  [/Step]

  [Step state:"algorithms"]
    # The Algorithms

    You may have noticed that its hard to find meaningful
    combinations of variable weights. This is something
    that computers are very good at: searching for
    some optimal combination of numbers.

    There are many algorithms that compute a dimensionality
    reduction of a dataset.


    [div style:`{display: 'flex', flexAlign: 'row', textAlign: 'center'}`]

      [button onClick:`algorithm = "pca" `]
        PCA
      [/button]

      [button onClick:`algorithm = "mctsne" `]
        TSNE
      [/button]

      [button onClick:`algorithm = "umap" `]
        UMAP
      [/button]
    [/div]

    [conditional if:`algorithm === 'pca' ` ]
      ## PCA

      Pros:

      Cons:
    [/conditional]
    [conditional if:`algorithm === 'mctsne' ` ]
      ## TSNE

      Pros:

      Cons:
    [/conditional]
    [conditional if:`algorithm === 'umap' ` ]
      ## UMAP

      Pros:

      Cons:


      [Cite reference:"maaten2008visualizing"  /]
    [/conditional]



  [/Step]

  [Step]
    # The Outro

    If you have a data set and wish to understand it better, there are a number of different algorithms and
    implementations to perform dimensionality reduction.
    In Python, the scikit-learn package provided implementions for
    [unsupervised dimensionality reduction](http://scikit-learn.org/stable/modules/unsupervised_reduction.html),
    as well as [manifold learning](http://scikit-learn.org/stable/modules/manifold.html): an approach to non-linear dimensionality reduction.

    ### Acknowledgements
  * This article was created using [Idyll](https://idyll-lang.org).
  * The source code is available on [Github](https://github.com/mathisonian/dimensionality-reduction).

    [References /]

  [/Step]

[/Scroller]



[ArtworkDetails artwork:selectedArtwork /]
