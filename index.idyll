[meta
  title:"What is Dimensionality Reduction?"
  description:"An intuitive guide to the statistical technique." /]


[data name:"images" source:"downloaded-artworks.csv"  /]
[var name:"scrollState" value:"loading" /]

[var name:"widthWeight" value:0.25 /]
[var name:"heightWeight" value:0.25 /]
[var name:"weightWeight" value:0.25 /]
[var name:"brightnessWeight" value:0.25 /]


[Scroller currentState:scrollState]

  [Graphic]
    [DRComponent
      images:images
      state:scrollState
      weights:`{
        'Width (cm)': widthWeight,
        'Height (cm)': heightWeight,
        'brightness_avg_perceived': brightnessWeight
      }`
       /]

  [/Graphic]

  [Step state:"loading"]
    [Header
      title:"What is Dimensionality Reduction?"
      date:"July 12, 2018"
      authors:`[
        { name: "Matthew Conlen", link: "https://twitter.com/mathisonian" },
        { name: "Fred Hohman", link: "http://twitter.com/fredhohman" }
      ]` /]

    [StartButton state:scrollState /]
  [/Step]

  [Step state:"reveal"]
    ## Example

    Let's start with an example.
    Consider a dataset of artworks from the [The Museum of Modern Art (MoMA) Collection](https://github.com/MuseumofModernArt/collection).
    This dataset contains 134,455 records, representing all of the works that have been accessioned into MoMA’s collection and cataloged in their database.
    Each artwork includes basic metadata, such as its title, artist, date made, medium, dimensions, and date acquired by the Museum.
    That means each artowkr is represntation by a number of different *features*.

    *Show table—introduce rows (data points) and columns (features)*.
  [/Step]

  [Step]

    // intro
    [Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) is a common yet powerful technique used in data mining, machine learning, and now more broadly, AI research and applications.
    The goal of dimsionality reduction, a subset of unsupervised learning, is to infer a function to describe the structure of "unlabeled" data, i.e. data that has no categorization.
    This presents a challenge for all dimsionality reduction techniques, since considered data is unlabelled, there is no straightforward way to evaluate the accuracy of the embedding or structure that is produced by the reduction algorithm.

    *need more motivation examples: why do you do DR? preprocessing, feature reduction, more important but non interpretable features*

    We can describe the technique using a data-centric view.
    Consider a datseta typically represented as a matrix
    [Equation display:false]X[/Equation], where
    [Equation display:false]X[/Equation]
    is of size
    [Equation display:false]m \times n[/Equation], where
    [Equation display:false]m[/Equation] is the number of rows of
    [Equation display:false]X[/Equation], and
    [Equation display:false]n[/Equation] is the number of columns.
    Typically, the rows are *data points* and the columns are *features*.
    Dimesionality reduction will reduce the number of eatures of each data point, turning
    [Equation display:false]X[/Equation] into a new matrix,
    [Equation display:false]X'[/Equation], of size
    [Equation display:false]m \times d[/Equation], where
    [Equation display:false]d < n[/Equation].

    For example, say [Equation display:false]m=n[/Equation], that is
    [Equation display:false]X[/Equation] is a square matrix.
    Performing dimesionality reduction on
    [Equation display:false]X[/Equation] will change it from a square matrix to a tall, thin, rectangular matrix.

    [br /]
    [Equation display:true]
    X =
    \begin{bmatrix}
    x & x & x \\
    x & x & x \\
    x & x & x
    \end{bmatrix}
    \implies
    \begin{bmatrix}
    x & x \\
    x & x \\
    x & x
    \end{bmatrix}
    = X'
    [/Equation]

    [br /]
    *Reducing a 3x3 square matrix to a 3x2 matrix. Each data point only has two features now, i.e., each point has been reduced from a 3 dimensional vector to a 2 dimensional vector.*
    [br /]

    [Cite reference:"test1"  /]

    [Cite reference:"test2"  /]

    [Cite reference:`["test1", "test2"]`  /]


  [/Step]

  [Step state:"1d"]
    Wooo 1D and stuff!
  [/Step]


  [Step state:"hilbert-brightness"]
    Hilbert space.

    *Q. Should we draw the hilbert curve in the background?*
  [/Step]

  [Step state:"hilbert-custom"]
    Can I interest you in your own projection?

    Use these sliders to toggle the influence of variables:

    *Width*
    [Range value:widthWeight min:0 max:1 step:0.005 /]

    *Height*
    [Range value:heightWeight min:0 max:1 step:0.005 /]

    // *Weight*
    // [Range value:weightWeight min:0 max:1 step:0.005 /]

    *Brightness*
    [Range value:brightnessWeight min:0 max:1 step:0.005 /]
  [/Step]

  [Step state:"pca"]
    Introduce real D.R. algorithms...
  [/Step]

  [Step]

    If you have a data set and wish to understand it better, there are a number of different algorithms and implementations to perform dimensionality reduction.
    In Python, the scikit-learn package provided implementions for [unsupervised dimensionality reduction](http://scikit-learn.org/stable/modules/unsupervised_reduction.html), as well as [manifold learning](http://scikit-learn.org/stable/modules/manifold.html): an approach to non-linear dimensionality reduction.

    ### Acknowledgements
* This article was created using [Idyll](https://idyll-lang.org).
* The source code is available on [Github](https://github.com/mathisonian/dimensionality-reduction).

    [References /]

  [/Step]

[/Scroller]

